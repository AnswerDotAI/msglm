{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from msglm import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# msglm\n",
    "\n",
    "> msglm makes it a little easier to create messages for language models like Claude and OpenAI GPTs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the latest version from pypi\n",
    "\n",
    "```sh\n",
    "$ pip install msglm\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use an LLM we need to structure our messages in a particular format. \n",
    "\n",
    "Here's an example of a text chat from the OpenAI docs.\n",
    "\n",
    "```python\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-4o\",\n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": \"What's the Wild Atlantic Way?\"}\n",
    "  ]\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating the correct format for a particular API can get tedious. The goal of *msglm* is to make it easier.\n",
    "\n",
    "The examples below will show you how to use *msglm* for text and image chats with OpenAI and Anthropic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Chats\n",
    "For a text chat simply pass a list of strings and the api format (e.g. \"openai\") to **mk_msgs** and it will generate the correct format.\n",
    "\n",
    "```python\n",
    "mk_msgs([\"Hello, world!\", \"some assistant response\"], api=\"openai\")\n",
    "```\n",
    "\n",
    "\n",
    "```js\n",
    "[\n",
    "    {\"role\": \"user\", \"content\": \"Hello, world!\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Some assistant response\"}\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### anthropic\n",
    "\n",
    "```python\n",
    "from msglm import mk_msgs_anthropic as mk_msgs\n",
    "from anthropic import Anthropic\n",
    "client = Anthropic()\n",
    "\n",
    "r = client.messages.create(\n",
    "    model=\"claude-3-haiku-20240307\",\n",
    "    max_tokens=1024,\n",
    "    messages=[mk_msgs([\"Hello, world!\", \"some LLM response\"])]\n",
    ")\n",
    "print(r.content[0].text)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### openai\n",
    "\n",
    "```python\n",
    "from msglm import mk_msgs_openai as mk_msgs\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "r = client.chat.completions.create(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  messages=[mk_msgs([\"Hello, world!\", \"some LLM response\"])]\n",
    ")\n",
    "print(r.choices[0].message.content)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Chats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an image chat simply pass the raw image bytes in a list with your question to *mk_msgs* and it will generate the correct format.\n",
    "\n",
    "\n",
    "```python\n",
    "mk_msg([img, \"What's in this image?\"], api=\"anthropic\")\n",
    "```\n",
    "\n",
    "```js\n",
    "[\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"source\": {\"type\": \"base64\", \"media_type\": media_type, \"data\": img}}\n",
    "            {\"type\": \"text\", \"text\": \"What's in this image?\"}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### anthropic\n",
    "\n",
    "```python\n",
    "import httpx\n",
    "from msglm import mk_msg_anthropic as mk_msg\n",
    "from anthropic import Anthropic\n",
    "\n",
    "client = Anthropic()\n",
    "\n",
    "img_url = \"https://www.atshq.org/wp-content/uploads/2022/07/shutterstock_1626122512.jpg\"\n",
    "img = httpx.get(img_url).content\n",
    "\n",
    "r = client.messages.create(\n",
    "    model=\"claude-3-haiku-20240307\",\n",
    "    max_tokens=1024,\n",
    "    messages=[mk_msg([img, \"Describe the image\"])]\n",
    ")\n",
    "print(r.content[0].text)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import httpx\n",
    "from msglm import mk_msg_openai as mk_msg\n",
    "from openai import OpenAI\n",
    "\n",
    "img_url = \"https://www.atshq.org/wp-content/uploads/2022/07/shutterstock_1626122512.jpg\"\n",
    "img = httpx.get(img_url).content\n",
    "\n",
    "client = OpenAI()\n",
    "r = client.chat.completions.create(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  messages=[mk_msg([img, \"Describe the image\"])]\n",
    ")\n",
    "print(r.choices[0].message.content)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API Wrappers\n",
    "To make life a little easier, msglm comes with api specific wrappers for `mk_msg` and `mk_msgs`.\n",
    "\n",
    "For Anthropic use \n",
    "\n",
    "```python \n",
    "from msglm import mk_msg_anthropic as mk_msg, mk_msgs_anthropic as mk_msgs\n",
    "```\n",
    "\n",
    "For OpenAI use\n",
    "\n",
    "```python\n",
    "from msglm import mk_msg_openai as mk_msg, mk_msgs_openai as mk_msgs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other use-cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt Caching\n",
    "*msglm* supports [prompt caching](https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching) for Anthropic models. Simply pass *cache=True* to *mk_msg* or *mk_msgs*.\n",
    "\n",
    "```python\n",
    "from msglm import mk_msg_anthropic as mk_msg\n",
    "\n",
    "mk_msg(\"please cache my message\", cache=True)\n",
    "```\n",
    "\n",
    "This generates the expected cache block below\n",
    "\n",
    "```js\n",
    "{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [\n",
    "        {\"type\": \"text\", \"text\": \"Please cache my message\", \"cache_control\": {\"type\": \"ephemeral\"}}\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PDF chats\n",
    "*msglm* offers PDF [support](https://docs.anthropic.com/en/docs/build-with-claude/pdf-support) for Anthropic. Just like an image chat all you need to do is pass the raw pdf bytes in a list with your question to *mk_msg* and it will generate the correct format as shown in the example below.\n",
    " \n",
    "```python\n",
    "import httpx\n",
    "from msglm import mk_msg_anthropic as mk_msg\n",
    "from anthropic import Anthropic\n",
    "\n",
    "client = Anthropic(default_headers={'anthropic-beta': 'pdfs-2024-09-25'})\n",
    "\n",
    "url = \"https://assets.anthropic.com/m/1cd9d098ac3e6467/original/Claude-3-Model-Card-October-Addendum.pdf\"\n",
    "pdf = httpx.get(url).content\n",
    "\n",
    "r = client.messages.create(\n",
    "    model=\"claude-3-5-sonnet-20241022\",\n",
    "    max_tokens=1024,\n",
    "    messages=[mk_msg([pdf, \"Which model has the highest human preference win rates across each use-case?\"])]\n",
    ")\n",
    "print(r.content[0].text)\n",
    "```\n",
    "\n",
    "Note: this feature is currently in beta so you'll need to:\n",
    "\n",
    " - use the Anthropic beta client (e.g. `anthropic.Anthropic(default_headers={'anthropic-beta': 'pdfs-2024-09-25'})`)\n",
    "  - use the `claude-3-5-sonnet-20241022` model  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "We hope *msglm* will make your life a little easier when chatting to LLMs. To learn more about the package please read this [doc](https://answerdotai.github.io/msglm/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
